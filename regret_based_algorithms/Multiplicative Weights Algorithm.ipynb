{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Multiplicative Weights Algorithm - Online Learning\n",
    "\n",
    "(Based on the notes from [Tim Roughgarden](http://timroughgarden.org/w16/l/l11.pdf)'s course).\n",
    "\n",
    "In this notebook we implement the multiplicative weights algortihm (MWA.)\n",
    "However it is known by many names (AdaBoost, Winnow, etc.) do to its rediscovery across multiple disciplines. \n",
    "Its is an online learning algorthm, that is it is used in the following online decision-making model:  \n",
    "\n",
    "Over a time horizion $t=1, 2, \\ldots, T$:\n",
    "- A player picks a probability distribution $p^t$ over actions $A$ at time $t$.\n",
    "- An adversary picks a a reward vector $r^t:A \\rightarrow [-1,1]$\n",
    "- Using $p^t$, an action $a^t$ is chosen and is given a reward $r^t(a^t)$.\n",
    "- The player then learns $r^t$. \n",
    "\n",
    "The goal of the player is to maximize their total reward.\n",
    "Due to the adversary's choice of $r^t$ it is clear that, in hindsight, no online algorithm could achieve the best sequence of actions. \n",
    "That is we cannot match the sequence of actions that will generate the maximum reward.\n",
    "Thus we move our goal posts: instead of comparinging algorithm to the best possible sequence of actions, we compare it to the best *fixed* sequence of actions. \n",
    "That is, instead of comparing our algorithm against $\\sum_{t=1}^T \\max_{a \\in A} r^t(a)$, we compare it to $\\max_{a \\in A}\\sum_{t=1}^T r^t(a)$.  \n",
    "\n",
    "This motivates the following definition of **regret**. For fixed reward vectors $r^1, \\ldots, r^T$, the regret of an action sequence $a^1, \\ldots, a^T$ is:  \n",
    "<center>\n",
    "$\n",
    "\\begin{align*}\n",
    "    \\max_{a \\in A} \\sum_{t=1}^T r^t(a) - \\sum_{t=1}^Tr^t(a^t)\n",
    "\\end{align*}\n",
    "$\n",
    "</center>\n",
    "\n",
    "\n",
    "Instead of maximizing our total reward, we would like to minimize our total regret, ideally to 0.\n",
    "As we've assumed our rewards per action are contained within $[-1,1]$, note that over a time horizion of $T$, our total regret is bounded with $[0, T]$.\n",
    "\n",
    "One idea for an algorithm is to track the rewards each action has recieved over the previous $t-1$ time steps, then at time $t$ choose the action that has achieved maximum acumulative reward. \n",
    "Such an algorithm is typically called **follow-the-leader**.\n",
    "However it can be shown that such an algorithm will achieve a linear regret (i.e. is $O(T)$ over $T$ timesteps - see Tim's notes.)\n",
    "\n",
    "The issue is that follow-the-leader is determinisitc and thus can be exploited by an adversary. \n",
    "Therefore we turn to a randomized algorithm - the MW algorithm.\n",
    "The MW algorithm has to guiding principles:\n",
    "1) The past performance of actions (via the amount of reward the generate) should guide the current action; this should be done probabilistically with probabilities proportional to cummulative reward per action.\n",
    "\n",
    "2) We should decrease the probability of choosing bad actions (those which yield low rewards) at an exponential rate. \n",
    "\n",
    "These principles are need to obtain regret that is sublinear in $T$ and to say that the MW algorithm actually achieves an optimal regret bounud (see Tim's notes.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
