{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regret Matching with Rock, Paper, Scissors\n",
    "---\n",
    "### Regret\n",
    "For a group of players let $s_i$ be the action player by player $i$ and $s_{-i}$ the actions played by the remaining players.\n",
    "Together, these actions form an action profile $a \\in A$.\n",
    "For all other actions $s_i'$ we could've played, we define the **regret of not playing action $s'_i$** as a difference in utility:  \n",
    "\n",
    "<center>\n",
    "$\n",
    "\\begin{align*}\n",
    "    regret(s_i', a) &= u(s_i', s_{i-1}) - u(a)\n",
    "\\end{align*}\n",
    "$\n",
    "</center>  \n",
    "\n",
    "For example, if we play scissors and our opponent plays rock, then $a=(scissors, rock)$ and our utility for this play is $u(scissors, rock) = -1$.\n",
    "We can compute the regret for all of our possible actions to find:  \n",
    "<center>\n",
    "$\n",
    "\\begin{align*}\n",
    "    regret(rock, a) &= u(rock, rock) - u((scissors, rock)) = 0 - (-1) = 1 \\\\\n",
    "    regret(paper, a) &= u(paper, rock) - u((scissors, rock)) = 1 - (-1) = 2 \\\\\n",
    "    regret(scissors, a) &= u(scissors, rock) - u((scissors, rock)) = -1 - (-1) = 0\n",
    "\\end{align*}\n",
    "$  \n",
    "</center>  \n",
    "\n",
    "Thus we regret not playing \"paper\" the most, and regret not playing \"rock\" more than playing \"scissors\".Note that when $s_i' = s_i$ the regret is zero. \n",
    "\n",
    "---\n",
    "### Regret Matching\n",
    "\n",
    "Actions that have positive regret is an indicator that we should've chosen these actions to maximize our utility. \n",
    "Thus if we track the regret for each action, if we choose actions at random with probability proportional to how positive their regret is, we can hopefully maximize our utility. Actions with negative regret are given zero probability.\n",
    "Such a weighting is called **regret matching**.  \n",
    "\n",
    "If we play another game, using the above regrets we play \"paper\" with probability $\\frac{2}{3}$ and \"rock\" with probability $\\frac{1}{3}$. Suppose we play \"paper\" while our opponent plays \"scissors\". \n",
    "Thus our regret for this game is:\n",
    "<center>\n",
    "$\n",
    "\\begin{align*}\n",
    "    regret(rock, a) &= u(rock, paper) - u((paper, scissors)) = -1 - (-1) = 0 \\\\\n",
    "    regret(paper, a) &= u(paper, paper) - u((paper, scissors)) = 0 - (-1) = 1 \\\\\n",
    "    regret(scissors, a) &= u(scissors, paper) - u((paper, scissors)) = 1 - (-1) = 2\n",
    "\\end{align*}\n",
    "$  \n",
    "</center>  \n",
    "\n",
    "If we add these regrets to our previous regrets, we can compute the **cumulative regrets** of $(1,3,2)$ respectively, which is normalized to $(\\frac{1}{6},\\frac{3}{6},\\frac{2}{6})$.\n",
    "These normalized weights form a mixed-strategy that can be used for the next game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Example RPS:\n",
    "We given an example of regret matching against an opponent that plays rock slightly more than paper or scissors.\n",
    "We initialize the cumulative regret of our agent to 0.\n",
    "Over several iterations, our agent will choose rock, paper, or scissors with a probability proportional to their cummulative regret via a strategy porfile. We then update their regrets using this action and repeat.  \n",
    "\n",
    "Regret Matching Algorithm:  \n",
    "`  \n",
    "Initialize cummulative regret to 0\n",
    "For some number of training iterations:\n",
    "    - Use cummulative regret to define strategy profile\n",
    "    - Add strategy profile to cummulative strategy profile (will use average after training.)\n",
    "    - Agent selects action according to strategy profile.\n",
    "    - Compute agent's regret, given opponents action.\n",
    "    - Update cummulative regret.\n",
    "Normalize cummulative strategy profile by number of training iterations.\n",
    "Return normalized strategy profile. \n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-71-3f5ef5c14edb>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-71-3f5ef5c14edb>\"\u001b[1;36m, line \u001b[1;32m8\u001b[0m\n\u001b[1;33m    global regret_sum = np.array([0, 0, 0])\u001b[0m\n\u001b[1;37m                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import random as rd \n",
    "import numpy as np\n",
    "\n",
    "ROCK, PAPER, SCISSORS = 0, 1, 2\n",
    "# Payoff matrix, First index is our agent's choice, Second the opponents choice\n",
    "PM = np.array([[0, -1, 1], [1, 0, -1], [-1, 1, 0]])\n",
    "\n",
    "regret_sum = np.array([0, 0, 0])\n",
    "strategy_sum = np.array([0, 0, 0])\n",
    "\n",
    "agent_strategy = np.array([0, 0, 0])\n",
    "oppo_strategy = np.array([0.5, 0.25, 0.25])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agent_strategy():\n",
    "    \"\"\"\n",
    "    If agent has positive regret for any action, we\n",
    "    \"\"\"\n",
    "    normalize = 0\n",
    "    # Set agent's strategy proportional to regret,\n",
    "    # uniform if regret is 0. \n",
    "    for a in range(3):\n",
    "        agent_strategy[a] = regret_sum[a] if regret_sum[a] > 0 else 0\n",
    "        normalize += agent_strategy[a]\n",
    "    \n",
    "    for a in range(3):\n",
    "        if (normalize > 0):\n",
    "            agent_strategy[a] /= normalize\n",
    "        else:\n",
    "            agent_strategy[a] = 1 / 3\n",
    "    # Update running strategy\n",
    "    for a in strategy_sum:\n",
    "        strategy_sum = [sum(x) for x in zip(strategy_sum, agent_strategy)]\n",
    "\n",
    "def get_action(strategy):\n",
    "    \"\"\"\n",
    "        Given strategy, generate rdnum in [0,1],\n",
    "        Return action corresponding to bin this number\n",
    "        falls into. \n",
    "    \"\"\"\n",
    "    r = rd.uniform(0,1)\n",
    "    \n",
    "    if r < strategy[0]:\n",
    "        return ROCK\n",
    "    elif r < strategy[1]+strategy[0]:\n",
    "        return PAPER\n",
    "    else:\n",
    "        return SCISSORS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs):\n",
    "    for i in range(n_epochs):\n",
    "        # Define strategy profile, update strategy_sum\n",
    "        get_agent_strategy()\n",
    "        # Get player actions, compute regret\n",
    "        agent_action = get_action(agent_strategy)\n",
    "        oppo_action = get_action(oppo_strategy)\n",
    "        \n",
    "        # Update regret\n",
    "        for a in range(3):\n",
    "            regret_sum[a] += PM[a][agent_action] - PM[agent_action][oppo_action]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'strategy_sum' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-e95873d1e6f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-69-df33fc30dc50>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(n_epochs)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[1;31m# Define strategy profile, update strategy_sum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mget_agent_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[1;31m# Get player actions, compute regret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0magent_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent_strategy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-68-e98675983180>\u001b[0m in \u001b[0;36mget_agent_strategy\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0magent_strategy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m# Update running strategy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstrategy_sum\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mstrategy_sum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstrategy_sum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent_strategy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'strategy_sum' referenced before assignment"
     ]
    }
   ],
   "source": [
    "train(500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
