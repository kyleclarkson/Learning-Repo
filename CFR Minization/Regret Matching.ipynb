{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regret Matching with Rock, Paper, Scissors\n",
    "---\n",
    "### Regret\n",
    "For a group of players let $s_i$ be the action player by player $i$ and $s_{-i}$ the actions played by the remaining players.\n",
    "Together, these actions form an action profile $a \\in A$.\n",
    "For all other actions $s_i'$ we could've played, we define the **regret of not playing action $s'_i$** as a difference in utility:  \n",
    "\n",
    "<center>\n",
    "$\n",
    "\\begin{align*}\n",
    "    regret(s_i', a) &= u(s_i', s_{i-1}) - u(a)\n",
    "\\end{align*}\n",
    "$\n",
    "</center>  \n",
    "\n",
    "For example, if we play scissors and our opponent plays rock, then $a=(scissors, rock)$ and our utility for this play is $u(scissors, rock) = -1$.\n",
    "We can compute the regret for all of our possible actions to find:  \n",
    "<center>\n",
    "$\n",
    "\\begin{align*}\n",
    "    regret(rock, a) &= u(rock, rock) - u((scissors, rock)) = 0 - (-1) = 1 \\\\\n",
    "    regret(paper, a) &= u(paper, rock) - u((scissors, rock)) = 1 - (-1) = 2 \\\\\n",
    "    regret(scissors, a) &= u(scissors, rock) - u((scissors, rock)) = -1 - (-1) = 0\n",
    "\\end{align*}\n",
    "$  \n",
    "</center>  \n",
    "\n",
    "Thus we regret not playing \"paper\" the most, and regret not playing \"rock\" more than playing \"scissors\".Note that when $s_i' = s_i$ the regret is zero. \n",
    "\n",
    "---\n",
    "### Regret Matching\n",
    "\n",
    "Actions that have positive regret is an indicator that we should've chosen these actions to maximize our utility. \n",
    "Thus if we track the regret for each action, if we choose actions at random with probability proportional to how positive their regret is, we can hopefully maximize our utility. Actions with negative regret are given zero probability.\n",
    "Such a weighting is called **regret matching**.  \n",
    "\n",
    "If we play another game, using the above regrets we play \"paper\" with probability $\\frac{2}{3}$ and \"rock\" with probability $\\frac{1}{3}$. Suppose we play \"paper\" while our opponent plays \"scissors\". \n",
    "Thus our regret for this game is:\n",
    "<center>\n",
    "$\n",
    "\\begin{align*}\n",
    "    regret(rock, a) &= u(rock, paper) - u((paper, scissors)) = -1 - (-1) = 0 \\\\\n",
    "    regret(paper, a) &= u(paper, paper) - u((paper, scissors)) = 0 - (-1) = 1 \\\\\n",
    "    regret(scissors, a) &= u(scissors, paper) - u((paper, scissors)) = 1 - (-1) = 2\n",
    "\\end{align*}\n",
    "$  \n",
    "</center>  \n",
    "\n",
    "If we add these regrets to our previous regrets, we can compute the **cumulative regrets** of $(1,3,2)$ respectively, which is normalized to $(\\frac{1}{6},\\frac{3}{6},\\frac{2}{6})$.\n",
    "These normalized weights form a mixed-strategy that can be used for the next game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Example RPS:\n",
    "We given an example of regret matching against an opponent that plays rock slightly more than paper or scissors.\n",
    "We initialize the cumulative regret of our agent to 0.\n",
    "Over several iterations, our agent will choose rock, paper, or scissors with a probability proportional to their cummulative regret via a strategy porfile. We then update their regrets using this action and repeat.  \n",
    "\n",
    "Regret Matching Algorithm:  \n",
    "`  \n",
    "Initialize cummulative regret to 0\n",
    "For some number of training iterations:\n",
    "    - Use cummulative regret to define strategy profile\n",
    "    - Add strategy profile to cummulative strategy profile (will use average after training.)\n",
    "    - Agent selects action according to strategy profile.\n",
    "    - Compute agent's regret, given opponents action.\n",
    "    - Update cummulative regret.\n",
    "Normalize cummulative strategy profile by number of training iterations.\n",
    "Return normalized strategy profile. \n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rd \n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RPS:\n",
    "    \n",
    "    ROCK, PAPER, SCISSORS = 0, 1, 2\n",
    "    N_ACTIONS = 3\n",
    "    # Payoff matrix, First index is our agent's choice, Second the opponents choice\n",
    "    util = np.array([[0, -1, 1], [1, 0, -1], [-1, 1, 0]])\n",
    "\n",
    "    def __init__(self, oppo_strat=np.array([1/3, 1/3, 1/3]), use_softmax=False):\n",
    "        self.use_softmax = use_softmax\n",
    "        self.reset(oppo_strat)\n",
    "        \n",
    "    def reset(self, oppo_strat):\n",
    "        self.regret_sum = np.array([0.0, 0.0, 0.0])\n",
    "        self.strategy_sum = np.array([0.0, 0.0, 0.0])\n",
    "\n",
    "        self.agent_strategy = np.array([0.0, 0.0, 0.0])\n",
    "        self.oppo_strategy = oppo_strat\n",
    "        \n",
    "    def set_agent_strategy(self):\n",
    "        if self.use_softmax:\n",
    "            # Set agent's strategy proportional to current regret, or uniform if all are non positive.\n",
    "            normalize_factor = 0\n",
    "            for a in range(self.N_ACTIONS):\n",
    "                self.agent_strategy[a] = self.regret_sum[a] if self.regret_sum[a] > 0 else 0\n",
    "                normalize_factor += self.agent_strategy[a]\n",
    "\n",
    "            for a in range(self.N_ACTIONS):\n",
    "                if (normalize_factor > 0):\n",
    "                    self.agent_strategy[a] /= normalize_factor\n",
    "                else:\n",
    "                    self.agent_strategy[a] = 1 / self.N_ACTIONS\n",
    "        else:\n",
    "            # Set agent's strategy proportional to current regret via softmax.\n",
    "            exp = np.exp(self.regret_sum)\n",
    "            self.agent_strategy = exp / np.sum(exp)\n",
    "    \n",
    "    def update_running_strategy(self):\n",
    "        self.strategy_sum += self.agent_strategy\n",
    "        \n",
    "    def get_action(self, strategy):\n",
    "        \n",
    "        # Given strategy, generate rdnum in [0,1], return action corresponding to bin this number falls into. \n",
    "        r = rd.uniform(0,1)\n",
    "\n",
    "        if r < strategy[0]:\n",
    "            return self.ROCK\n",
    "        elif r < strategy[1]+strategy[0]:\n",
    "            return self.PAPER\n",
    "        else:\n",
    "            return self.SCISSORS\n",
    "        \n",
    "    def print_arrays(self):\n",
    "        # For debugging. \n",
    "        print(f'Regret sum: {self.regret_sum}')\n",
    "        print(f'Strategy sum: {self.strategy_sum}')\n",
    "        print(f'Agent Strategy: {self.agent_strategy}')\n",
    "        \n",
    "    def softmax(self, array):\n",
    "        x = np.exp(array)\n",
    "        return x / np.sum(x)\n",
    "        \n",
    "        \n",
    "    def train(self, n_epochs=10_000):\n",
    "        # Determine strategy for agent. \n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            self.set_agent_strategy()\n",
    "            self.update_running_strategy()\n",
    "            agent_action = self.get_action(self.agent_strategy)\n",
    "            oppo_action = self.get_action(self.oppo_strategy)\n",
    "            for i in range(self.N_ACTIONS):\n",
    "                self.regret_sum[i] += (self.util[i][oppo_action] - self.util[agent_action][oppo_action])\n",
    "        \n",
    "#             if epoch % 1000 == 0:\n",
    "#                 x = self.strategy_sum / epoch\n",
    "#                 print(f'Epoch {epoch}. Average strategy: {x}')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll attempt to learn an optimal strategy to play when our opponent has a preference for playing rock more than the other two options. We will run 10 experiments to see what strategy is generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50_000\n",
    "oppo_strat_rock = [0.5, 0.25, 0.25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Strategy: [0.00046486 0.99951195 0.00002319]\n",
      "Iteration 2: Strategy: [0.00017554 0.99971422 0.00011024]\n",
      "Iteration 3: Strategy: [0.00003328 0.99989098 0.00007574]\n",
      "Iteration 4: Strategy: [0.00001765 0.99994535 0.000037  ]\n",
      "Iteration 5: Strategy: [0.00050425 0.99945306 0.00004268]\n",
      "Iteration 6: Strategy: [0.00002327 0.99996027 0.00001646]\n",
      "Iteration 7: Strategy: [0.0001353  0.99969058 0.00017412]\n",
      "Iteration 8: Strategy: [0.00001273 0.99996128 0.00002599]\n",
      "Iteration 9: Strategy: [0.00001201 0.99992869 0.0000593 ]\n",
      "Iteration 10: Strategy: [0.00004421 0.99993374 0.00002206]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 11):\n",
    "    rps = RPS(oppo_strat_rock, use_softmax=False)\n",
    "    rps.train(n_epochs)\n",
    "    print(f'Iteration {i}: Strategy: {rps.strategy_sum / n_epochs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all iterations, the average strategy returned all suggest that we play paper most of the time. This is reasonable given the opponent's strategy: as they typically play rock, in the long run we will win the most games (and maximize our utility) by playing paper. \n",
    "\n",
    "Let's repeat this experiment but choose actions to play via a softmax function. Recall that we choose actions proportional to their regret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Strategy: [0.00191633 0.998067   0.00001667]\n",
      "Iteration 2: Strategy: [0.00000667 0.99998667 0.00000667]\n",
      "Iteration 3: Strategy: [0.00008333 0.99989    0.00002667]\n",
      "Iteration 4: Strategy: [0.00123229 0.99875438 0.00001333]\n",
      "Iteration 5: Strategy: [0.00006333 0.99993    0.00000667]\n",
      "Iteration 6: Strategy: [0.00065849 0.99871333 0.00062818]\n",
      "Iteration 7: Strategy: [0.00019977 0.99975023 0.00005   ]\n",
      "Iteration 8: Strategy: [0.0006033  0.99939004 0.00000667]\n",
      "Iteration 9: Strategy: [0.0002324  0.99969882 0.00006878]\n",
      "Iteration 10: Strategy: [0.00001333 0.99998    0.00000667]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 11):\n",
    "    rps = RPS(oppo_strat_rock, use_softmax=True)\n",
    "    rps.train(n_epochs)\n",
    "    print(f'Iteration {i}: Strategy: {rps.strategy_sum / n_epochs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we achieve similar results. Finally, let's see what happens when the opponent is clever and uses actions uniformly at random. Here, our regret matching is likely to fail us as their is no dominant strategy to play against this opponent that maximizes our utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Strategy: [0.4617967  0.11100432 0.42719899]\n",
      "Iteration 2: Strategy: [0.86425294 0.11852674 0.01722032]\n",
      "Iteration 3: Strategy: [0.05346663 0.91809169 0.02844169]\n",
      "Iteration 4: Strategy: [0.06794672 0.1074527  0.82460057]\n",
      "Iteration 5: Strategy: [0.07040969 0.02806498 0.90152533]\n",
      "Iteration 6: Strategy: [0.2899674  0.46907745 0.24095515]\n",
      "Iteration 7: Strategy: [0.74658699 0.00005238 0.25336062]\n",
      "Iteration 8: Strategy: [0.88282237 0.00003953 0.11713809]\n",
      "Iteration 9: Strategy: [0.00116568 0.30869513 0.69013919]\n",
      "Iteration 10: Strategy: [0.75313986 0.00355577 0.24330438]\n"
     ]
    }
   ],
   "source": [
    "oppo_strat_uniform = [1/3.0, 1/3.0, 1/3.0]\n",
    "for i in range(1, 11):\n",
    "    rps = RPS(oppo_strat_uniform, use_softmax=False)\n",
    "    rps.train(n_epochs)\n",
    "    print(f'Iteration {i}: Strategy: {rps.strategy_sum / n_epochs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected the resulting strategy does not favour a single action consistently as seen in the previous experiments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
