{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regret Matching with Rock, Paper, Scissors\n",
    "---\n",
    "### Regret\n",
    "For a group of players let $s_i$ be the action player by player $i$ and $s_{-i}$ the actions played by the remaining players.\n",
    "Together, these actions form an action profile $a \\in A$.\n",
    "For all other actions $s_i'$ we could've played, we define the **regret of not playing action $s'_i$** as a difference in utility:  \n",
    "\n",
    "<center>\n",
    "$\n",
    "\\begin{align*}\n",
    "    regret(s_i', a) &= u(s_i', s_{i-1}) - u(a)\n",
    "\\end{align*}\n",
    "$\n",
    "</center>  \n",
    "\n",
    "For example, if we play scissors and our opponent plays rock, then $a=(scissors, rock)$ and our utility for this play is $u(scissors, rock) = -1$.\n",
    "We can compute the regret for all of our possible actions to find:  \n",
    "<center>\n",
    "$\n",
    "\\begin{align*}\n",
    "    regret(rock, a) &= u(rock, rock) - u((scissors, rock)) = 0 - (-1) = 1 \\\\\n",
    "    regret(paper, a) &= u(paper, rock) - u((scissors, rock)) = 1 - (-1) = 2 \\\\\n",
    "    regret(scissors, a) &= u(scissors, rock) - u((scissors, rock)) = -1 - (-1) = 0\n",
    "\\end{align*}\n",
    "$  \n",
    "</center>  \n",
    "\n",
    "Thus we regret not playing \"paper\" the most, and regret not playing \"rock\" more than playing \"scissors\".Note that when $s_i' = s_i$ the regret is zero. \n",
    "\n",
    "---\n",
    "### Regret Matching\n",
    "\n",
    "Actions that have positive regret is an indicator that we should've chosen these actions to maximize our utility. \n",
    "Thus if we track the regret for each action, if we choose actions at random with probability proportional to how positive their regret is, we can hopefully maximize our utility. Actions with negative regret are given zero probability.\n",
    "Such a weighting is called **regret matching**.  \n",
    "\n",
    "If we play another game, using the above regrets we play \"paper\" with probability $\\frac{2}{3}$ and \"rock\" with probability $\\frac{1}{3}$. Suppose we play \"paper\" while our opponent plays \"scissors\". \n",
    "Thus our regret for this game is:\n",
    "<center>\n",
    "$\n",
    "\\begin{align*}\n",
    "    regret(rock, a) &= u(rock, paper) - u((paper, scissors)) = -1 - (-1) = 0 \\\\\n",
    "    regret(paper, a) &= u(paper, paper) - u((paper, scissors)) = 0 - (-1) = 1 \\\\\n",
    "    regret(scissors, a) &= u(scissors, paper) - u((paper, scissors)) = 1 - (-1) = 2\n",
    "\\end{align*}\n",
    "$  \n",
    "</center>  \n",
    "\n",
    "If we add these regrets to our previous regrets, we can compute the **cumulative regrets** of $(1,3,2)$ respectively, which is normalized to $(\\frac{1}{6},\\frac{3}{6},\\frac{2}{6})$.\n",
    "These normalized weights form a mixed-strategy that can be used for the next game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Example RPS:\n",
    "We given an example of regret matching against an opponent that plays rock slightly more than paper or scissors.\n",
    "We initialize the cumulative regret of our agent to 0.\n",
    "Over several iterations, our agent will choose rock, paper, or scissors with a probability proportional to their cummulative regret via a strategy porfile. We then update their regrets using this action and repeat.  \n",
    "\n",
    "Regret Matching Algorithm:  \n",
    "`  \n",
    "Initialize cummulative regret to 0\n",
    "For some number of training iterations:\n",
    "    - Use cummulative regret to define strategy profile\n",
    "    - Add strategy profile to cummulative strategy profile (will use average after training.)\n",
    "    - Agent selects action according to strategy profile.\n",
    "    - Compute agent's regret, given opponents action.\n",
    "    - Update cummulative regret.\n",
    "Normalize cummulative strategy profile by number of training iterations.\n",
    "Return normalized strategy profile. \n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rd \n",
    "import numpy as np\n",
    "\n",
    "ROCK, PAPER, SCISSORS = 0, 1, 2\n",
    "N_ACTIONS = 3\n",
    "# Payoff matrix, First index is our agent's choice, Second the opponents choice\n",
    "PM = np.array([[0, -1, 1], [1, 0, -1], [-1, 1, 0]])\n",
    "\n",
    "regret_sum = np.array([0, 0, 0])\n",
    "global strategy_sum\n",
    "strategy_sum = np.array([0, 0, 0])\n",
    "\n",
    "agent_strategy = np.array([0, 0, 0])\n",
    "oppo_strategy = np.array([0.5, 0.25, 0.25])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(strategy):\n",
    "    \"\"\"\n",
    "        Given strategy, generate rdnum in [0,1],\n",
    "        Return action corresponding to bin this number\n",
    "        falls into. \n",
    "    \"\"\"\n",
    "    r = rd.uniform(0,1)\n",
    "    \n",
    "    if r < strategy[0]:\n",
    "        return ROCK\n",
    "    elif r < strategy[1]+strategy[0]:\n",
    "        return PAPER\n",
    "    else:\n",
    "        return SCISSORS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs):\n",
    "    for i in range(n_epochs):\n",
    "        \n",
    "        # Set agent's strategy proportional to current regret, or uniform if all regrets is non positive. \n",
    "        normalize_factor = 0\n",
    "        for a in range(N_ACTIONS):\n",
    "            agent_strategy[a] = regret_sum[a] if regret_sum[a] > 0 else 0\n",
    "            normalize_factor += agent_strategy[a]\n",
    "            \n",
    "        for a in range(N_ACTIONS):\n",
    "            if (normalize_factor > 0):\n",
    "                agent_strategy[a] /= normalize_factor\n",
    "            else:\n",
    "                agent_strategy[a] = 1 / N_ACTIONS\n",
    "                \n",
    "        # Update running strategy sum.\n",
    "        strategy_sum += agent_strategy\n",
    "        # Get player actions, compute regret\n",
    "        agent_action = get_action(agent_strategy)\n",
    "        oppo_action = get_action(oppo_strategy)\n",
    "        \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'strategy_sum' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-e95873d1e6f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-bdda77eed5dd>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(n_epochs)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;31m# Update running strategy sum.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mstrategy_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0magent_strategy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[1;31m# Get player actions, compute regret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0magent_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent_strategy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'strategy_sum' referenced before assignment"
     ]
    }
   ],
   "source": [
    "train(500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
