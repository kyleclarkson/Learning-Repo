{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Neural Network for Temperature Data\n",
    "\n",
    "In this notebook we construct a neural network consisitng of a single hidden layer to represent our function. As the unknown measurements are actually degrees in Fahrenheit and the conversion between it and Celsius is given by a linear function, it is likely that the resulting model will overfit to the data and will represent a nonlinear function.  \n",
    "\n",
    "None the less this notebook serves as an introduction to using PyTorch's `nn` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kyle\\Anaconda3\\envs\\rl\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "C:\\Users\\Kyle\\Anaconda3\\envs\\rl\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# Our data (again)\n",
    "# temperatures in unknown units\n",
    "temp_x = [0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0]\n",
    "# temperatures in celsius. \n",
    "temp_y = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
    "assert(len(temp_x) == len(temp_y))\n",
    "\n",
    "# Create tensors for data\n",
    "t_x = T.tensor(temp_x)\n",
    "t_y = T.tensor(temp_y)\n",
    "\n",
    "# Add 1th dimension to input data. These is to comply with PyTorch's\n",
    "# batches expectation with input data (0th dim. states batch size.)\n",
    "t_x = torch.tensor(t_x).unsqueeze(1)\n",
    "t_y = torch.tensor(t_y).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=1, out_features=15, bias=True)\n",
      "  (1): Tanh()\n",
      "  (2): Linear(in_features=15, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create model with a single hidden layer with 15 neurons\n",
    "\n",
    "seq_model = nn.Sequential(\n",
    "    nn.Linear(1, 15),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(15, 1))\n",
    "\n",
    "print(seq_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.0899],\n",
       "         [-0.6910],\n",
       "         [-0.0044],\n",
       "         [ 0.0567],\n",
       "         [-0.6128],\n",
       "         [ 0.8493],\n",
       "         [-0.6056],\n",
       "         [ 0.4051],\n",
       "         [-0.4936],\n",
       "         [ 0.4525],\n",
       "         [ 0.1326],\n",
       "         [-0.6039],\n",
       "         [-0.6878],\n",
       "         [-0.7831],\n",
       "         [-0.5342]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0792, -0.4598,  0.5173, -0.7771, -0.3062, -0.3127, -0.8256,  0.9550,\n",
       "          0.3146,  0.8134, -0.5455, -0.6401,  0.0209,  0.1148,  0.0512],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.2390,  0.1181, -0.1329,  0.1910, -0.0730,  0.0116, -0.1690, -0.0893,\n",
       "           0.1201,  0.0527,  0.0099, -0.0867,  0.0754, -0.2494, -0.0755]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.0222], requires_grad=True)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iterate over all parameters in the model\n",
    "[param for param in seq_model.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([15, 1]), torch.Size([15]), torch.Size([1, 15]), torch.Size([1])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[param.shape for param in seq_model.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_linear.weight torch.Size([15, 1])\n",
      "hidden_linear.bias torch.Size([15])\n",
      "output_linear.weight torch.Size([1, 15])\n",
      "output_linear.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "We can name the layers of our network by using an Ordered Dictonary.\n",
    "\"\"\"\n",
    "from collections import OrderedDict\n",
    "\n",
    "seq_model = nn.Sequential(OrderedDict([\n",
    "    ('hidden_linear', nn.Linear(1, 15)),\n",
    "    ('hidden_activation', nn.Tanh()),\n",
    "    ('output_linear', nn.Linear(15, 1))\n",
    "]))\n",
    "\n",
    "for name, param in seq_model.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Having created out model, we pass it to a similar training loop as before. \n",
    "\"\"\"\n",
    "\n",
    "def train(n_epochs, optimizer, model ,loss_fn, x, y_label):\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y_label)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 5000 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000, Loss 45.6920\n",
      "Epoch 2000, Loss 67.9159\n",
      "Epoch 3000, Loss 69.8201\n",
      "Epoch 4000, Loss 44.2211\n",
      "Epoch 5000, Loss 48.3799\n",
      "Epoch 6000, Loss 73.0020\n",
      "Epoch 7000, Loss 43.5817\n",
      "Epoch 8000, Loss 72.1019\n",
      "Epoch 9000, Loss 43.8656\n",
      "Epoch 10000, Loss 51.5240\n",
      "Epoch 11000, Loss 48.9969\n",
      "Epoch 12000, Loss 59.2284\n",
      "Epoch 13000, Loss 81.9125\n",
      "Epoch 14000, Loss 52.7329\n",
      "Epoch 15000, Loss 59.7242\n",
      "Epoch 16000, Loss 46.5802\n",
      "Epoch 17000, Loss 71.9937\n",
      "Epoch 18000, Loss 75.6322\n",
      "Epoch 19000, Loss 44.5222\n",
      "Epoch 20000, Loss 48.5677\n",
      "Epoch 21000, Loss 50.0033\n",
      "Epoch 22000, Loss 46.3954\n",
      "Epoch 23000, Loss 49.0176\n",
      "Epoch 24000, Loss 44.1672\n",
      "Epoch 25000, Loss 46.8693\n",
      "Epoch 26000, Loss 44.9130\n",
      "Epoch 27000, Loss 47.2667\n",
      "Epoch 28000, Loss 55.3407\n",
      "Epoch 29000, Loss 49.8544\n",
      "Epoch 30000, Loss 88.3560\n",
      "Epoch 31000, Loss 44.8903\n",
      "Epoch 32000, Loss 78.7738\n",
      "Epoch 33000, Loss 43.9730\n",
      "Epoch 34000, Loss 73.1445\n",
      "Epoch 35000, Loss 81.3536\n",
      "Epoch 36000, Loss 46.0322\n",
      "Epoch 37000, Loss 60.3253\n",
      "Epoch 38000, Loss 44.7976\n",
      "Epoch 39000, Loss 44.9898\n",
      "Epoch 40000, Loss 51.0845\n",
      "Epoch 41000, Loss 62.1544\n",
      "Epoch 42000, Loss 45.9266\n",
      "Epoch 43000, Loss 54.6970\n",
      "Epoch 44000, Loss 59.8064\n",
      "Epoch 45000, Loss 46.4052\n",
      "Epoch 46000, Loss 48.8911\n",
      "Epoch 47000, Loss 69.9215\n",
      "Epoch 48000, Loss 44.5650\n",
      "Epoch 49000, Loss 48.6693\n",
      "Epoch 50000, Loss 45.8962\n",
      "Epoch 51000, Loss 59.4332\n",
      "Epoch 52000, Loss 60.7991\n",
      "Epoch 53000, Loss 44.7163\n",
      "Epoch 54000, Loss 45.4253\n",
      "Epoch 55000, Loss 70.5961\n",
      "Epoch 56000, Loss 45.8143\n",
      "Epoch 57000, Loss 48.8429\n",
      "Epoch 58000, Loss 51.6907\n",
      "Epoch 59000, Loss 56.5201\n",
      "Epoch 60000, Loss 46.5468\n",
      "Epoch 61000, Loss 44.6401\n",
      "Epoch 62000, Loss 47.0400\n",
      "Epoch 63000, Loss 50.1841\n",
      "Epoch 64000, Loss 46.4854\n",
      "Epoch 65000, Loss 51.0749\n",
      "Epoch 66000, Loss 44.8966\n",
      "Epoch 67000, Loss 43.3693\n",
      "Epoch 68000, Loss 53.4752\n",
      "Epoch 69000, Loss 44.5385\n",
      "Epoch 70000, Loss 72.0893\n"
     ]
    }
   ],
   "source": [
    "optimizer  = optim.SGD(seq_model.parameters(), 0.001)\n",
    "\n",
    "train(70000, optimizer, seq_model, nn.MSELoss(), t_x, t_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
